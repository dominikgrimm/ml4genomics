{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Example: Ridge Regression vs. SVM\n",
    "<p></p>\n",
    "    \n",
    "<div style=\"text-align:justify\">\n",
    "    In this toy example we will compare two machine learning models: <em>Ridge Regression</em> and <em>C-SVM</em>. The data is generated <em>in silico</em> and is only used to illustrate how to use <em>Ridge Regression</em> and <em>C-SVM</em>.\n",
    "</div>\n",
    "\n",
    "## Problem Description of the Toy Example\n",
    "<p></p>\n",
    "    \n",
    "<div style=\"text-align:justify\">\n",
    "A new cancer drug was developed for therapy. During the clinical trail the researchers releaized that the drug had a faster response for a certain subgroup of the patients, while it was less responsive in the others. In addition, the researchers recognized that the drug leads to severe side-effects the longer the patient is treated with the drug. The goal should be to reduce the side effects by treating only those patients that are predicted to have a fast response when taking the drug.\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"text-align:justify\">\n",
    "The researches believe that different genetic mutations in the genomes of the individual patients might play a role for the differences in response times.\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"text-align:justify\">\n",
    "    The researches contacted the <em>machine learning</em> lab to build a predictive model. The model should predict the individual response time of the drug based on the individual genetic backgrounds of a patient.\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"text-align:justify\">\n",
    "For this purpose, we get a dataset of 400 patients. For each patient a panel of 600 genetic mutations was measured. In addition, the researchers measured how many days it took until the drug showed a positive response.\n",
    "</div>\n",
    "\n",
    "\n",
    "## 1. Using Ridge Regression to predict the response time\n",
    "<div style=\"text-align:justify\">\n",
    "    To predict the response time of the drug for new patients, we will train a <em>Ridge Regression</em> model. The target variable for this task is the response time in days. The features are the 600 genetic mutations measured for each of the 400 patients. To avoid overfitting we will use a nested-crossvalidation to determine the optimal hyperparamter.\n",
    "</div>\n",
    "### 1.1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy as sp\n",
    "import matplotlib\n",
    "import pylab as pl\n",
    "matplotlib.rcParams.update({'font.size': 15})\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV,StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "def visualized_variance_bias_tradeoff(hyperp, line_search, optimal_hyperp,classification=False):\n",
    "    pl.figure(figsize=(18,7))\n",
    "    if classification:\n",
    "        factor=1\n",
    "    else:\n",
    "        factor=-1\n",
    "    pl.plot(hyperp,line_search.cv_results_['mean_train_score']*factor,label=\"Training Error\",color=\"#e67e22\")\n",
    "    pl.fill_between(hyperp,\n",
    "                    line_search.cv_results_['mean_train_score']*factor-line_search.cv_results_['std_train_score'],\n",
    "                    line_search.cv_results_['mean_train_score']*factor+line_search.cv_results_['std_train_score'],\n",
    "                    alpha=0.3,color=\"#e67e22\")\n",
    "    pl.plot(hyperp,line_search.cv_results_['mean_test_score']*factor,label=\"Validation Error\",color=\"#2980b9\")\n",
    "    pl.fill_between(hyperp,\n",
    "                    line_search.cv_results_['mean_test_score']*factor-line_search.cv_results_['std_test_score'],\n",
    "                    line_search.cv_results_['mean_test_score']*factor+line_search.cv_results_['std_test_score'],\n",
    "                    alpha=0.3,color=\"#2980b9\")\n",
    "    pl.xscale(\"log\")\n",
    "    if classification:\n",
    "        pl.ylabel(\"Accuracy\")\n",
    "    else:\n",
    "        pl.ylabel(\"Mean Squared Error\")\n",
    "    pl.xlabel(\"Hyperparameter\")\n",
    "    pl.legend(frameon=True)\n",
    "    pl.grid(True)\n",
    "    pl.axvline(x=optimal_hyperp,color='r',linestyle=\"--\")\n",
    "    pl.title(\"Training- vs. Validation-Error (Optimal Hyperparameter = %.1e)\"%optimal_hyperp);\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "#TODO: Load Data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Summary of the Data\n",
    "print(\"Orginal Data\")\n",
    "print(\"Number Patients:\\t%d\"%data.shape[0])\n",
    "print(\"Number Features:\\t%d\"%data.shape[1])\n",
    "print()\n",
    "\n",
    "#TODO: Split Data into Training and Testing data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training Data\")\n",
    "print(\"Number Patients:\\t%d\"%training_data.shape[0])\n",
    "print(\"Number Features:\\t%d\"%training_data.shape[1])\n",
    "print()\n",
    "print(\"Testing Data\")\n",
    "print(\"Number Patients:\\t%d\"%testing_data.shape[0])\n",
    "print(\"Number Features:\\t%d\"%testing_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Train Ridge Regression on training data\n",
    "\n",
    "The first step is to train the ridge regression model on the training data with a **5-fold cross-validation** with an **internal line-search** to find the **optimal hyperparameter $\\alpha$**. We will plot the **training errors** against the **validation errors**, to illustrate the effect of different $\\alpha$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Initialize different alpha values for the Ridge Regression model\n",
    "alphas = sp.logspace(-2,8,11)\n",
    "param_grid = dict(alpha=alphas)\n",
    "\n",
    "##TODO: 5-fold cross-validation (outer-loop)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##TODO: Line-search to find the optimal alpha value (internal-loop)\n",
    "##TODO: Model performance is measured with the negative mean squared error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##TODO: Execute nested cross-validation and compute mean squared error\n",
    "\n",
    "\n",
    "\n",
    "print(\"5-fold nested cross-validation\")\n",
    "print(\"Mean-Squared-Error:\\t\\t%.2f (-+ %.2f)\"%(score.mean()*(-1),score.std()))\n",
    "print\n",
    "\n",
    "##TODO: Estimate optimal alpha on the full training data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Visualize training and validation error for different alphas\n",
    "visualized_variance_bias_tradeoff(alphas, line_search, optimal_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Train Ridge Regression with optimal $\\alpha$ and evaluate model in test data\n",
    "Next we retrain the ridge regresssion model with the optimal $\\alpha$ (from the last section). After re-training we will test the model on the not used test data to evaluate the model performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Train Ridge Regression on the full training data with optimal alpha\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##TODO: Use trained model the predict new instances in test data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Prediction results on test data\")\n",
    "print(\"MSE (test data, alpha=optimal):\\t%.2f \"%(mean_squared_error(testing_target,predictions)))\n",
    "print(\"Optimal Alpha:\\t\\t\\t%.2f\"%optimal_alpha)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align:justify\">\n",
    "    Using 5-fold cross-validation on the training data leads to a mean squared error (MSE) of $MSE=587.09 \\pm 53.54$. On the test data we get an error of $MSE=699.56$ ($\\sim 26.5$ days). That indicates that the ridge regression model performs rather mediocre (even with hyperparameter optimization).\n",
    "    One reason might be that the target variable (number of days until the drug shows a positive response) is insufficently described by the given features (genetic mutations).\n",
    "</div>\n",
    "\n",
    "\n",
    "## 2. Prediction of patients with slow and fast response times using a Support-Vector-Machine  \n",
    "\n",
    "<div style=\"text-align:justify\">\n",
    "    Due to the rather bad results with the ridge regession model the machine learning lab returned to the researchers to discuss potential issues. The researches than mentioned that it might not be necessarily important to predict the exact number of days. It might be even better to only predict if a patient reacts fast or slowly on the drug. Based on some prior experiments the researchers observed, that most of the patients showed severe side-effects after 50 days of treatment. Thus we can binarise the data, such that all patients below 50 days are put into class 0 and all others into class 1. This leads to a classical classification problem for which a support vector machine could be used. \n",
    "</div>\n",
    "\n",
    "### 2.1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Split data into training and testing splits, stratified by class-ratios\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training Data\")\n",
    "print(\"Number Patients:\\t\\t%d\"%training_data.shape[0])\n",
    "print(\"Number Features:\\t\\t%d\"%training_data.shape[1])\n",
    "print(\"Number Patients Class 0:\\t%d\"%(training_target==0).sum())\n",
    "print(\"Number Patients Class 1:\\t%d\"%(training_target==1).sum())\n",
    "print()\n",
    "print(\"Testing Data\")\n",
    "print(\"Number Patients:\\t\\t%d\"%testing_data.shape[0])\n",
    "print(\"Number Features:\\t\\t%d\"%testing_data.shape[1])\n",
    "print(\"Number Patients Class 0:\\t%d\"%(testing_target==0).sum())\n",
    "print(\"Number Patients Class 1:\\t%d\"%(testing_target==1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.2 Classification with a linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Initialize hyperparameter space for C\n",
    "Cs = sp.logspace(-7, 1, 9)\n",
    "param_grid = dict(C=Cs)\n",
    "\n",
    "#TODO: Implement Grid search for SVC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##TODO: Perform 5 Fold cross-validation with internal line-search and report average Accuracy\n",
    "\n",
    "\n",
    "\n",
    "print(\"5-fold nested cross-validation on training data\")\n",
    "print(\"Average(Accuracy):\\t\\t\\t%.2f (-+ %.2f)\"%(score.mean(),score.std()))\n",
    "print()\n",
    "\n",
    "#TODO retrain on full training data\n",
    "grid.fit(training_data,training_target)\n",
    "optimal_C = grid.best_params_['C']\n",
    "\n",
    "#Plot variance bias tradeoff\n",
    "visualized_variance_bias_tradeoff(Cs, grid, optimal_C,classification=True)\n",
    "\n",
    "##TODO: retrain model with optimal C and evaluate on test data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Prediction with optimal C\")\n",
    "print(\"Accuracy (Test data, C=Optimal):\\t%.2f \"%(accuracy_score(testing_target,predictions)))\n",
    "print(\"Optimal C:\\t\\t\\t\\t%.2e\"%optimal_C)\n",
    "print()\n",
    "\n",
    "#TODO: Compute ROC FPR, TPR and AUC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Plot ROC Curve\n",
    "pl.figure(figsize=(8,8))\n",
    "pl.plot(fpr, tpr, color='darkorange',\n",
    "         lw=3, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "pl.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
    "pl.xlim([-0.01, 1.0])\n",
    "pl.ylim([0.0, 1.05])\n",
    "pl.xlabel('False Positive Rate (1-Specificity)',fontsize=18)\n",
    "pl.ylabel('True Positive Rate (Sensitivity)',fontsize=18)\n",
    "pl.title('Receiver Operating Characteristic (ROC) Curve',fontsize=18)\n",
    "pl.legend(loc=\"lower right\",fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Classification with SVM and RBF kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize search space for C and gamma values\n",
    "Cs = sp.logspace(-4, 4, 9)\n",
    "gammas = sp.logspace(-7, 1, 9)\n",
    "param_grid = dict(C=Cs,gamma=gammas)\n",
    "\n",
    "#TODO: implement SVM using RBF kernel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##TODO: Perform 5 Fold cross-validation with internal line-search and report average Accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"5-fold nested cross-validation on training data\")\n",
    "print(\"Average(Accuracy):\\t\\t\\t%.2f (-+ %.2f)\"%(score.mean(),score.std()))\n",
    "print()\n",
    "\n",
    "#TODO: Retrain with optimal parameters and test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Prediction with optimal C and Gamma\")\n",
    "print(\"Accuracy (Test Data, C=Optimal):\\t%.2f \"%(accuracy_score(testing_target,predictions)))\n",
    "print(\"Optimal C:\\t\\t\\t\\t%.2e\"%optimal_C)\n",
    "print(\"Optimal Gamma:\\t\\t\\t\\t%.2e\"%optimal_gamma)\n",
    "print()\n",
    "\n",
    "\n",
    "#TODO: Compute ROC FPR, TPR and AUC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Plot ROC Curve\n",
    "pl.figure(figsize=(8,8))\n",
    "pl.plot(fpr, tpr, color='darkorange',\n",
    "         lw=3, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "pl.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
    "pl.xlim([-0.01, 1.0])\n",
    "pl.ylim([0.0, 1.05])\n",
    "pl.xlabel('False Positive Rate (1-Specificity)',fontsize=18)\n",
    "pl.ylabel('True Positive Rate (Sensitivity)',fontsize=18)\n",
    "pl.title('Receiver Operating Characteristic (ROC) Curve',fontsize=18)\n",
    "pl.legend(loc=\"lower right\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
